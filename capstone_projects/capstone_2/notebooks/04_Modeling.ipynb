{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Modeling \n",
    "\n",
    "Contents\n",
    "\n",
    "4.1 [Introduction](#4.1)\n",
    "\n",
    "  * [4.1.1 Problem Recap](#4.1.1)\n",
    "  * [4.1.2 Notebook Goals](#4.1.2)\n",
    " \n",
    "4.2 [Load the data](#4.2)\n",
    "\n",
    "  * [4.2.1 Imports](#4.2.1)\n",
    "  * [4.2.2 Load the data](#4.2.2)\n",
    "\n",
    "4.3 [Examine Class Split](#4.3)\n",
    "\n",
    "4.4 [Pre-processing](#4.4)\n",
    "\n",
    "  * [4.4.1 Set Random Seed for Reproducability](#4.4.1)\n",
    "  * [4.4.2 Train/test Split](#4.4.2)\n",
    "  * [4.4.4 Examine Class Split for Train/Test Data](#4.4.4)\n",
    "  \n",
    "\n",
    "4.5 [Setting Up Pipelines](#4.5)\n",
    "  * 4.5.1 [Previous Best Model: Logistic Regression with Count Vectorization](#4.5.1)\n",
    "<br/><br/>\n",
    "    * [4.5.1.1 Training and Fitting the Model](#4.5.1.1)\n",
    "    * [4.5.1.2 Evaluating the Model](#4.5.1.2)\n",
    "<br/><br/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9c841",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.1 Introduction <a name=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d02b97",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.1 Problem Recap <a name=\"4.1.1\"><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadbd87",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using customer text data about amazon products, we will build, evaluate and compare models to estimate the probability that a given text review can be classified as “positive” or “negative”.\n",
    "\n",
    "Our goal is to build a text classifier using Amazon product review data which can be used to analyze customer sentiment which does not have accompanying numeric data. The metric we will be primarily interested in will be Recall on the positive class. This is the proportion of the positive class (negative reviews coded as \"1\" in the data) we correctly predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc4983",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.2 Notebook Goals <a name=\"4.1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda4730",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. In our previous notebook our best results came from Term-Frequency Inverse-Document Frequency vectorization and a Logistic Regression Model.\n",
    "\n",
    "2. We had slightly worse results from a Naive Bayes and Random Forest model. The Naive Bayes model incorrectly predicted a higher proportion of the negative class and the Random Forest model appeared to strongly overfit the training data with a very poor Recall on the test set.\n",
    "\n",
    "3. Try over-sampling the minority class that we are trying to predict (encoded as \"1\"s) and/or under-sampling the majority class.\n",
    "\n",
    "4. Test some other models such as gradient boosted trees (LightGBM/XGBoost) \n",
    "\n",
    "5. Examine how well our models will generalize with K-fold cross validation.\n",
    "\n",
    "6. Tune hyper-parameters with grid-search or bayesian search optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d646fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2 Load the data <a name=\"4.2\"><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f718e2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2.1 Imports <a name=\"4.2.1\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589120c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/pyspark/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/paul/anaconda3/envs/pyspark/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "\n",
    "#reading/processing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "#splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#scaling/vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from imblearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "#dealing with class imbalance\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#hyper-parameter tuning\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041448da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2.2 Load the data <a name=\"4.2.2\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d6a53a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pq.read_table(\"../data/edited/fashion.parquet\")\n",
    "fashion = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, sub_sample_prop):\n",
    "\n",
    "    # select vectorization parameters\n",
    "    \n",
    "\n",
    "\n",
    "    #sampler\n",
    "    sampler_type = trial.suggest_categorical('sampler', [None, 'ros', 'rus', 'smote', 'ada'])\n",
    "\n",
    "    if sampler_type == 'ros':\n",
    "        sampler = RandomOverSampler(random_state=0)\n",
    "    \n",
    "    elif sampler_type == 'smote':\n",
    "        k_neighbors = trial.suggest_int('k_neighbors', 2,10)\n",
    "        sampler = SMOTE(random_state=0, k_neighbors=k_neighbors)\n",
    "    \n",
    "    elif sampler_type == 'rus':\n",
    "        sampler = RandomUnderSampler(random_state=0)\n",
    "    \n",
    "    elif sampler_type == 'ada':\n",
    "        n_neighbors = trial.suggest_int('n_neighbors', 2,10)\n",
    "        sampler = ADASYN(n_neighbors=n_neighbors)\n",
    "    \n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "\n",
    "    model_type = trial.suggest_categorical('classifier', ['XGBClassifier'])\n",
    "\n",
    "    if model_type == 'LogisticRegression':\n",
    "        #optimize params\n",
    "        C = trial.suggest_categorical('C', [1.0, 0.1, 0.01]) #note: models with larger values for C failed to converge\n",
    "        \n",
    "        #model\n",
    "        model = LogisticRegression(solver = \"lbfgs\", n_jobs=-1, max_iter=1000, C=C)\n",
    "\n",
    "    elif model_type == 'XGBClassifier':\n",
    "        #optimize params\n",
    "        learning_rate = trial.suggest_categorical('learning_rate', [0.2, 0.1, 0.01, .001, .0001])\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        n_estimators = trial.suggest_categorical('n_estimators', [200,500,1000, 2000, 4000])\n",
    "\n",
    "        #model\n",
    "        model = xgb.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, n_jobs=-1, verbosity=0, use_label_encoder=False)\n",
    "\n",
    "    else:\n",
    "        #optimize params\n",
    "        learning_rate = trial.suggest_categorical('learning_rate', [0.2, 0.1, 0.01, .001, .0001])\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        n_estimators = trial.suggest_categorical('n_estimators', [200,500,1000, 2000,4000])\n",
    "\n",
    "        #model\n",
    "        model = lgb.LGBMClassifier(max_depth = max_depth, n_estimators=n_estimators)\n",
    "    \n",
    "    pipeline = Pipeline([('sampler', sampler), ('model',model)])\n",
    "    \n",
    "    X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=sub_sample_prop)\n",
    "\n",
    "    pipeline.fit(X_train_sample, y_train_sample)\n",
    "        \n",
    "    y_preds = pipeline.predict(X_test)\n",
    "\n",
    "    return recall_score(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-12 10:49:41,395]\u001b[0m A new study created in memory with name: no-name-ceef3b97-1ccf-458e-b75b-abdba51e7b5f\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 10:58:15,653]\u001b[0m Trial 0 finished with value: 0.7586171381488602 and parameters: {'sampler': 'smote', 'k_neighbors': 4, 'classifier': 'XGBClassifier', 'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 4000}. Best is trial 0 with value: 0.7586171381488602.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 10:59:38,000]\u001b[0m Trial 1 finished with value: 0.752669944058315 and parameters: {'sampler': 'ada', 'n_neighbors': 10, 'classifier': 'XGBClassifier', 'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}. Best is trial 0 with value: 0.7586171381488602.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:00:12,195]\u001b[0m Trial 2 finished with value: 0.7298604825369082 and parameters: {'sampler': 'rus', 'classifier': 'XGBClassifier', 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 2000}. Best is trial 0 with value: 0.7586171381488602.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:01:39,835]\u001b[0m Trial 3 finished with value: 0.685253880167374 and parameters: {'sampler': 'rus', 'classifier': 'XGBClassifier', 'learning_rate': 0.01, 'max_depth': 13, 'n_estimators': 1000}. Best is trial 0 with value: 0.7586171381488602.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:05:51,149]\u001b[0m Trial 4 finished with value: 0.7790783455466483 and parameters: {'sampler': None, 'classifier': 'XGBClassifier', 'learning_rate': 0.001, 'max_depth': 18, 'n_estimators': 1000}. Best is trial 4 with value: 0.7790783455466483.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:19:09,091]\u001b[0m Trial 5 finished with value: 0.5739486860196643 and parameters: {'sampler': 'smote', 'k_neighbors': 9, 'classifier': 'XGBClassifier', 'learning_rate': 0.0001, 'max_depth': 20, 'n_estimators': 2000}. Best is trial 4 with value: 0.7790783455466483.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:19:22,403]\u001b[0m Trial 6 finished with value: 0.6658867992441285 and parameters: {'sampler': 'ros', 'classifier': 'XGBClassifier', 'learning_rate': 0.0001, 'max_depth': 6, 'n_estimators': 200}. Best is trial 4 with value: 0.7790783455466483.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:23:22,164]\u001b[0m Trial 7 finished with value: 0.83451259450936 and parameters: {'sampler': None, 'classifier': 'XGBClassifier', 'learning_rate': 0.1, 'max_depth': 14, 'n_estimators': 2000}. Best is trial 7 with value: 0.83451259450936.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:27:40,607]\u001b[0m Trial 8 finished with value: 0.7273499526252164 and parameters: {'sampler': 'smote', 'k_neighbors': 5, 'classifier': 'XGBClassifier', 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 2000}. Best is trial 7 with value: 0.83451259450936.\u001b[0m\n",
      "\u001b[32m[I 2022-07-12 11:28:17,404]\u001b[0m Trial 9 finished with value: 0.7982206227820263 and parameters: {'sampler': None, 'classifier': 'XGBClassifier', 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000}. Best is trial 7 with value: 0.83451259450936.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df = 5, max_df=0.95)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(fashion[\"review\"].values, fashion[\"neg_sentiment\"], test_size = .1, random_state=1)\n",
    "\n",
    "y_train, y_test = np.ravel(y_train), np.ravel(y_test)\n",
    "\n",
    "y_train, y_test = y_train.astype(int), y_test.astype(int)\n",
    "\n",
    "tfidf.fit(X_train)\n",
    "X_train = tfidf.transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing comparison of various under/oversampling techniques\n",
    "\n",
    "In data sets with an imbalanced split between the classes we are trying to predict, there are few possible approaches to try to improve the target metric our model (classifier) is optimizing for.\n",
    "\n",
    "1. Over-sampling - if we train with a higher proportion of the class we are trying to predict using resampling, we may be able to improve the result for our classifier.\n",
    "\n",
    "2. Under-sampling - by the same logic, we can under-sample the majority classes we are NOT trying to predict.\n",
    "\n",
    "Both of these can introduce worse outcome metrics for our alternate classes, which may be an issue depending on the specific problem.\n",
    "\n",
    "\n",
    "3. Synthesize data: we can generate artificial data using the minority class we are trying to predict. ADASYN and SMOTE both use Nearest Neigbhors algorithms to generate artificial points that are located \"close\" in the n-dimensional feature space of the target class to the actual data points. Conceptually, we can think of it as if we gathered MORE data, and are assuming it looks similar to the current data we have. It will be unlikely to have strong outliers due to the nature of the algorithm and will be more \"clumped\" together than if we gathered more \"real\" data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical output of the various types of sampling/explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing comparison of various models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy comparison of various models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d73df188c3ed28f9117694899638a2078581279f5400990dc36b7edf848ab19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
