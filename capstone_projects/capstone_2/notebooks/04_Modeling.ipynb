{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Modeling \n",
    "\n",
    "Contents\n",
    "\n",
    "4.1 [Introduction](#4.1)\n",
    "\n",
    "  * [4.1.1 Problem Recap](#4.1.1)\n",
    "  * [4.1.2 Notebook Goals](#4.1.2)\n",
    " \n",
    "4.2 [Load the data](#4.2)\n",
    "\n",
    "  * [4.2.1 Imports](#4.2.1)\n",
    "  * [4.2.2 Load the data](#4.2.2)\n",
    "\n",
    "4.3 [Examine Class Split](#4.3)\n",
    "\n",
    "4.4 [Pre-processing](#4.4)\n",
    "\n",
    "  * [4.4.1 Set Random Seed for Reproducability](#4.4.1)\n",
    "  * [4.4.2 Train/test Split](#4.4.2)\n",
    "  * [4.4.4 Examine Class Split for Train/Test Data](#4.4.4)\n",
    "  \n",
    "\n",
    "4.5 [Setting Up Pipelines](#4.5)\n",
    "  * 4.5.1 [Previous Best Model: Logistic Regression with Count Vectorization](#4.5.1)\n",
    "<br/><br/>\n",
    "    * [4.5.1.1 Training and Fitting the Model](#4.5.1.1)\n",
    "    * [4.5.1.2 Evaluating the Model](#4.5.1.2)\n",
    "<br/><br/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9c841",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.1 Introduction <a name=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d02b97",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.1 Problem Recap <a name=\"4.1.1\"><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadbd87",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using customer text data about amazon products, we will build, evaluate and compare models to estimate the probability that a given text review can be classified as “positive” or “negative”.\n",
    "\n",
    "Our goal is to build a text classifier using Amazon product review data which can be used to analyze customer sentiment which does not have accompanying numeric data. The metric we will be primarily interested in will be Recall on the positive class. This is the proportion of the positive class (negative reviews coded as \"1\" in the data) we correctly predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc4983",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.2 Notebook Goals <a name=\"4.1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda4730",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. In our previous notebook our best results came from Term-Frequency Inverse-Document Frequency vectorization and a Logistic Regression Model.\n",
    "\n",
    "2. We had slightly worse results from a Naive Bayes and Random Forest model. The Naive Bayes model incorrectly predicted a higher proportion of the negative class and the Random Forest model appeared to strongly overfit the training data with a very poor Recall on the test set.\n",
    "\n",
    "3. Try over-sampling the minority class that we are trying to predict (encoded as \"1\"s) and/or under-sampling the majority class.\n",
    "\n",
    "4. Test some other models such as gradient boosted trees (LightGBM/XGBoost) \n",
    "\n",
    "5. Examine how well our models will generalize with K-fold cross validation.\n",
    "\n",
    "6. Tune hyper-parameters with grid-search or bayesian search optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d646fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2 Load the data <a name=\"4.2\"><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f718e2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2.1 Imports <a name=\"4.2.1\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589120c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/pyspark/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "\n",
    "#reading/processing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "#splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imblearn as im\n",
    "\n",
    "#scaling/vectorization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import word2vec, FastText\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#dealing with class imbalance\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#hyperparameter tuning\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041448da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2.2 Load the data <a name=\"4.2.2\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d6a53a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pq.read_table(\"../data/edited/fashion.parquet\")\n",
    "fashion = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # select vectorization parameters\n",
    "    \n",
    "\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), min_df = 5, max_df=0.95)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fashion[\"review\"].values, fashion[\"neg_sentiment\"], test_size = .1)\n",
    "\n",
    "    y_train, y_test = np.ravel(y_train), np.ravel(y_test)\n",
    "    y_train, y_test = y_train.astype(int), y_test.astype(int)\n",
    "\n",
    "    #sampler\n",
    "    sampler_type = trial.suggest_categorical('sampler', ['ros', 'rus', None, 'smote'])\n",
    "\n",
    "    if sampler_type == 'ros':\n",
    "        sampler = RandomOverSampler(random_state=0)\n",
    "    \n",
    "    elif sampler_type == 'smote':\n",
    "        k_neighbors = trial.suggest_int('k_neighbors', 2,5)\n",
    "        sampler = SMOTE(random_state=0, k_neighbors=k_neighbors)\n",
    "    \n",
    "    elif sampler_type == 'rus':\n",
    "        sampler = RandomUnderSampler(random_state=0)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    model_type = trial.suggest_categorical('classifier', ['XGBClassifier']) #'LGBMClassifier', 'LogisticRegression'\n",
    "\n",
    "    if model_type == 'LogisticRegression':\n",
    "\n",
    "        C = trial.suggest_categorical('C', [3, 1.0, 0.1, 0.01]) #note: models with larger values for C failed to converge\n",
    "        model = LogisticRegression(solver = \"lbfgs\", n_jobs=-1, max_iter=1000, C=C)\n",
    "\n",
    "    elif model_type == 'XGBClassifier':\n",
    "        \n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.1)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 2,10)\n",
    "\n",
    "        model = xgb.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, n_jobs=-1, random_state=0, verbosity=0, use_label_encoder=False)\n",
    "\n",
    "    else:\n",
    "              \n",
    "        model = LightGBM\n",
    "    \n",
    "    pipeline = Pipeline([('tfidf', tfidf), ('sampler', sampler), ('model',model)])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Fit worked\")\n",
    "\n",
    "    y_preds = pipeline.predict(X_test)\n",
    "\n",
    "    return recall_score(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 13:59:26,764]\u001b[0m A new study created in memory with name: no-name-d8130abc-6609-4a85-b0a3-c3793ea4018e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 13:59:44,070]\u001b[0m Trial 0 finished with value: 0.6794405948960622 and parameters: {'sampler': 'ros', 'classifier': 'XGBClassifier', 'learning_rate': 0.0968750508029098, 'max_depth': 6, 'n_estimators': 8}. Best is trial 0 with value: 0.6794405948960622.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:16:26,312]\u001b[0m Trial 1 finished with value: 0.6441725373872204 and parameters: {'sampler': 'smote', 'k_neighbors': 4, 'classifier': 'XGBClassifier', 'learning_rate': 0.055186723334697305, 'max_depth': 4, 'n_estimators': 7}. Best is trial 0 with value: 0.6794405948960622.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:16:44,221]\u001b[0m Trial 2 finished with value: 0.6814635800898838 and parameters: {'sampler': 'ros', 'classifier': 'XGBClassifier', 'learning_rate': 0.04403205019923962, 'max_depth': 7, 'n_estimators': 8}. Best is trial 2 with value: 0.6814635800898838.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:16:59,692]\u001b[0m Trial 3 finished with value: 0.7415875754961173 and parameters: {'sampler': None, 'classifier': 'XGBClassifier', 'learning_rate': 0.06040277859460945, 'max_depth': 9, 'n_estimators': 4}. Best is trial 3 with value: 0.7415875754961173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:17:19,040]\u001b[0m Trial 4 finished with value: 0.7608858297171929 and parameters: {'sampler': None, 'classifier': 'XGBClassifier', 'learning_rate': 0.08180149542123777, 'max_depth': 10, 'n_estimators': 9}. Best is trial 4 with value: 0.7608858297171929.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:17:34,812]\u001b[0m Trial 5 finished with value: 0.7103896711418815 and parameters: {'sampler': None, 'classifier': 'XGBClassifier', 'learning_rate': 0.06390275060274116, 'max_depth': 6, 'n_estimators': 7}. Best is trial 4 with value: 0.7608858297171929.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:17:49,434]\u001b[0m Trial 6 finished with value: 0.654938610958402 and parameters: {'sampler': 'ros', 'classifier': 'XGBClassifier', 'learning_rate': 0.051699603709427805, 'max_depth': 4, 'n_estimators': 4}. Best is trial 4 with value: 0.7608858297171929.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:18:06,227]\u001b[0m Trial 7 finished with value: 0.7006222441999423 and parameters: {'sampler': 'rus', 'classifier': 'XGBClassifier', 'learning_rate': 0.04545761035075771, 'max_depth': 10, 'n_estimators': 8}. Best is trial 4 with value: 0.7608858297171929.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-07 14:18:22,910]\u001b[0m Trial 8 finished with value: 0.6650706998389118 and parameters: {'sampler': 'ros', 'classifier': 'XGBClassifier', 'learning_rate': 0.03094175344296864, 'max_depth': 4, 'n_estimators': 9}. Best is trial 4 with value: 0.7608858297171929.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d73df188c3ed28f9117694899638a2078581279f5400990dc36b7edf848ab19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
